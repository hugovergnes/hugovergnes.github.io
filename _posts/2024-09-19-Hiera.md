---
title:  "Hierarchical Vision Transformers as Masked Autoencoders"
layout: post
categories: AI
---

In recent years, Vision Transformers ([ViT](https://arxiv.org/abs/2010.11929)) have garnered significant attention for their impressive performance in computer vision tasks. It challenged the traditional vision paradigm that relies on convolutions. The sequence-to-sequence paradigm is less intuitive for vision tasks compared to NLPs, but some architectures have proven to be undeniably strong at many vision tasks.


## The Challenges of ViT with high-resolution images

As the resolution of images increases, the computational and memory demands of ViTs become increasingly prohibitive. Because the number of tokens grows quadratically with the image resolution! This quickly becomes prohibitive. This is where Hierarchical Vision Transformers ([Hiera](https://arxiv.org/abs/2306.00989)) shine as a more efficient and scalable alternative.

Vision Transformers rely on dividing an image into fixed-size non-overlapping patches, which are then treated as tokens for the model. The key issue arises from the fact that each token is processed through all stages of the transformer. As a result, when working with high-resolution images, the number of tokens increases dramatically, leading to a massive increase in computational complexity and memory usage. This makes ViT less feasible for large images, where processing all tokens at every stage becomes impractical.

## How Hiera addresses these issues

Hiera, on the other hand, takes a hierarchical approach to token processing. Pooling the query tokens (Q) significantly improves efficiency. Unlike ViT, Hiera does not process all tokens at every stage. Instead, it selectively reduces the number of tokens passed through deeper layers. This hierarchical tokenization ensures that only the most relevant tokens are processed at higher levels, drastically reducing the memory footprint while maintaining or even improving model performance.

In high-resolution image tasks, this approach allows Hiera to maintain a more manageable number of tokens at each stage. By focusing on the most important information at each level, Hiera can capture fine-grained details without the need to process the entire set of tokens throughout the model. This makes Hiera a strong candidate for replacing ViT, especially when working with images of very high resolution.

## Further development for higher resolution images

However, many groups (Including me at my work) witnessed something unexpected. Hiera really doesn't generalizes well to different resolutions. This shouldn't happen, the only component of a Vision Transformers that require knowledge of the input resolution is the positional embedding. There is no reason for this to cause any problems to the higher resolution downstream task. It turns out the problem came out of an interaction between [the Window Attention and the positional embedding](https://arxiv.org/abs/2311.05613). Hiera was then used in the trunk of the image encoder for [SAM2](https://github.com/facebookresearch/segment-anything-2). It was one of the key contributions that enabled the creation of this compelling realtime demo.

This was an exciting innovation to see as it leads to great improvements. In a field where we need more players, we need more papers that made it possible for smaller group to propose SOTA systems even with less resources.

## In my experience. Pre-training, training and inference.

Hiera holds many promises as it addresses the prohibitive memory usage of non-hierarchical Vision Transformers. However, they still rely on a sequence to sequence paradigm that is not ideal for vision. In my experience, it appears that the inductive biases inherent to CNNs make for better models. Typically, ConvNext is a very hard model to beat in the downstream prediction tasks.

In my experience, Hiera undeniably beats ViT. But it is still underwhelming in many aspects. First, the MAE pre-training step is computationally expensive. This is a heavy constraint on relatively GPU poor groups. Second, the performance of the fine-tuning step is lackluster. Not only it still requires a decent amount of data prediction task, but the model struggles to be competitive with his more advanced counterparts (ConvNext, PvT, MViT, Swin-v2) that are initialized from ImageNet and trained directly on the task (No specific pretraining task). I haven't managed to make it really competitive despite my best efforts and extensive experimentation (Different masking ratio, layer wise learning rate decay, high batch size, many different fine-tuning settings). It gets better if you keep the earlier layers unchanged (Prefer downsampling the image over cropping, keep the patch size fixed between Pre-training and Fine-tuning, etc.) but it never becomes SOTA in the proprietary datasets I worked on. Third, it looks like Hiera has some convergence issues. The pre-training step might result in occasional crashes where the loss suddenly becomes NaN and all the weights are messed up. All of this for a speed increase in the forward and backward pass that is marginal for relatively high resolution images (~3000 * 2000).

## Closing thoughts

I think Hiera and MAE in general are interesting approaches. But they serve little utility when you try to apply it without some of the biggest datasets out there coupled with some serious GPU power. Nonetheless, I think pretext task for pre-training make sense and might help in many downstream scenarios. I'm excited to read more research on this topic.