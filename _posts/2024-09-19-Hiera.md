---
title:  "Hierarchical Vision Transformers as Masked Auto Encoders"
layout: post
categories: AI
---

In recent years, Vision Transformers ([ViT](https://arxiv.org/abs/2010.11929)) have garnered significant attention for their impressive performance in computer vision tasks. It challenged the paradigme of vision that uses convolutions. This sequence-to-sequence paradigme is less intuitive for Vision than it is to MLP but the over parametrisation of those networks makes them very powerful.




## The Challenges of ViT with High-Resolution Images

As the resolution of images increases, the computational and memory demands of ViTs become increasingly prohibitive. Because the number of tokens grows quadratically with the image resolution! This quickly becomes prohibitive. This is where Hierarchical Vision Transformers ([Hiera](https://arxiv.org/abs/2306.00989)) shine as a more efficient and scalable alternative.

Vision Transformers rely on dividing an image into fixed-size non-overlapping patches, which are then treated as tokens for the model. The key issue arises from the fact that each token is processed through all stages of the transformer. As a result, when working with high-resolution images, the number of tokens increases dramatically, leading to a massive increase in computational complexity and memory usage. This makes ViT less feasible for large images, where processing all tokens at every stage becomes impractical.

## How Hiera Addresses These Issues

Hiera, on the other hand, takes a hierarchical approach to token processing. Pooling the query tokens (Q) significantly improves efficiency. Unlike ViT, Hiera does not process all tokens at every stage. Instead, it selectively reduces the number of tokens passed through deeper layers. This hierarchical tokenization ensures that only the most relevant tokens are processed at higher levels, drastically reducing the memory footprint while maintaining or even improving model performance.

In high-resolution image tasks, this approach allows Hiera to maintain a more manageable number of tokens at each stage. By focusing on the most important information at each level, Hiera can capture fine-grained details without the need to process the entire set of tokens throughout the model. This makes Hiera a strong candidate for replacing ViT, especially when working with images of very high resolution.

## Further development for higher resolution images

However, many groups (Including me at my work) witnessed something unexpected. Hiera really doesn't generalizes well for a different
resolution. This shouldn't happen, the only components of a Vision Transformers that requires knowledge of the input resolution is the positional embedding. There is no reasin for this to cause any problems to the higher resolution downstream task. It turns out the problem came out of an interaction between [the Window Attention and the positional embedding](https://arxiv.org/abs/2311.05613). Hiera was then used in the trunk of the image encoder for [SAM2](https://github.com/facebookresearch/segment-anything-2).

This was an exciting innovation to see as it leads to great improvements. Personally, I have a soft spot for those papers that reduces the computational requirements of these algorithms. It's becoming tough even for start-ups with million of dollars in backing to train models from scratch! These types of innovations allievate these problems.